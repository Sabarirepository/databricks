The main differences between CREATE TABLE, CREATE STREAMING LIVE TABLE, and CREATE LIVE TABLE are the semantics and behavior associated with each type of table:

CREATE TABLE: This statement is used to create a regular, non-streaming table in Databricks. The table can be used to store and query data, but it does not have the ability to ingest streaming data. This type of table is typically used for batch processing or analytical workloads where data is loaded into the table at once.

CREATE STREAMING LIVE TABLE: This statement is used to create a streaming table in Databricks. A streaming table can ingest and process data in real-time from streaming sources such as Apache Kafka or Azure Event Hubs. These tables are used to handle continuous data streams and are suitable for real-time analytics or event-driven applications. Data is continuously appended to the streaming table as new events arrive.

CREATE LIVE TABLE: This statement is used to create a live table in Databricks. A live table is a combination of batch and streaming processing capabilities. It can handle both static data and data from streaming sources. Live tables provide the ability to perform real-time analytics on streaming data and join it with static/reference data. They support both batch processing and continuous processing on the streaming data. Live tables are useful for scenarios that require a combination of batch and streaming processing, such as maintaining up-to-date analytics or creating materialized views.

In summary, CREATE TABLE is used for regular batch processing, CREATE STREAMING LIVE TABLE is used for continuous ingestion and processing of streaming data, and CREATE LIVE TABLE is used for a combination of batch and streaming processing. The choice of which type to use depends on the nature of the data and the processing requirements of the workload.

CREATE STREAMING LIVE TABLE customers
COMMENT "The customers buying finished products, ingested from /databricks-datasets."
TBLPROPERTIES ("myCompanyPipeline.quality" = "mapping")
AS SELECT * FROM cloud_files("/databricks-datasets/retail-org/customers/", "csv");

CREATE STREAMING LIVE TABLE sales_orders_raw
COMMENT "The raw sales orders, ingested from /databricks-datasets."
TBLPROPERTIES ("myCompanyPipeline.quality" = "bronze")
AS
SELECT * FROM cloud_files("/databricks-datasets/retail-org/sales_orders/", "json", map("cloudFiles.inferColumnTypes", "true"))


CREATE STREAMING LIVE TABLE sales_orders_cleaned(
  CONSTRAINT valid_order_number EXPECT (order_number IS NOT NULL) ON VIOLATION DROP ROW
)
PARTITIONED BY (order_date)
COMMENT "The cleaned sales orders with valid order_number(s) and partitioned by order_datetime."
TBLPROPERTIES ("myCompanyPipeline.quality" = "silver")
AS
SELECT f.customer_id, f.customer_name, f.number_of_line_items, 
  TIMESTAMP(from_unixtime((cast(f.order_datetime as long)))) as order_datetime, 
  DATE(from_unixtime((cast(f.order_datetime as long)))) as order_date, 
  f.order_number, f.ordered_products, c.state, c.city, c.lon, c.lat, c.units_purchased, c.loyalty_segment
  FROM STREAM(LIVE.sales_orders_raw) f
  LEFT JOIN LIVE.customers c
      ON c.customer_id = f.customer_id
     AND c.customer_name = f.customer_name
	 
	 

CREATE LIVE TABLE sales_order_in_la
COMMENT "Sales orders in LA."
TBLPROPERTIES ("myCompanyPipeline.quality" = "gold")
AS
SELECT city, order_date, customer_id, customer_name, ordered_products_explode.curr, SUM(ordered_products_explode.price) as sales, SUM(ordered_products_explode.qty) as quantity, COUNT(ordered_products_explode.id) as product_count
FROM (
  SELECT city, order_date, customer_id, customer_name, EXPLODE(ordered_products) as ordered_products_explode
  FROM LIVE.sales_orders_cleaned 
  WHERE city = 'Los Angeles'
  )
GROUP BY order_date, city, customer_id, customer_name, ordered_products_explode.curr




CREATE LIVE TABLE sales_order_in_chicago
COMMENT "Sales orders in Chicago."
TBLPROPERTIES ("myCompanyPipeline.quality" = "gold")
AS
SELECT city, order_date, customer_id, customer_name, ordered_products_explode.curr, SUM(ordered_products_explode.price) as sales, SUM(ordered_products_explode.qty) as quantity, COUNT(ordered_products_explode.id) as product_count
FROM (
  SELECT city, order_date, customer_id, customer_name, EXPLODE(ordered_products) as ordered_products_explode
  FROM LIVE.sales_orders_cleaned 
  WHERE city = 'Chicago'
  )
GROUP BY order_date, city, customer_id, customer_name, ordered_products_explode.curr



CREATE STREAMING LIVE TABLE sensor_data_stream (
  sensor_id STRING,
  temperature DOUBLE,
  humidity DOUBLE,
  timestamp TIMESTAMP
)
USING org.apache.spark.sql.pubsub
OPTIONS (
  subscription "projects/my-project/subscriptions/my-subscription",
  format "cloudPubSub",
  valueFormat "json"
)
COMMENT "Streaming table for sensor data"
TBLPROPERTIES ("myCompanyPipeline.quality" = "silver");



CREATE STREAMING LIVE TABLE sensor_data_stream (
  sensor_id STRING,
  temperature DOUBLE,
  humidity DOUBLE,
  timestamp TIMESTAMP
)
USING org.apache.spark.sql.kafka010
OPTIONS (
  kafka.bootstrap.servers "kafka-server:9092",
  subscribe "sensor_data_topic",
  startingOffsets "earliest",
  failOnDataLoss "false"
)
COMMENT "Streaming table for sensor data from Kafka"
TBLPROPERTIES ("myCompanyPipeline.quality" = "silver");

-----------------
%python
from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder.getOrCreate()

# Read the CSV file into a DataFrame
df = spark.read.format("csv").option("header", "true").load("/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv")

# Convert DataFrame to Delta table
df.write.format("delta").mode("overwrite").save("/tmp/diamonds")

# Define the new data to be upserted
new_data = spark.createDataFrame([(1, 'A', 'new color', 99)], ['carat', 'cut', 'color', 'price'])

# Perform upsert operation
delta_table = spark.read.format("delta").load("/tmp/diamonds")
delta_table.createOrReplaceTempView("diamonds")

upsert_query = """
  MERGE INTO diamonds
  USING delta_changes
  ON (diamonds.carat = delta_changes.carat AND diamonds.cut = delta_changes.cut)
  WHEN MATCHED THEN
    UPDATE SET diamonds.color = delta_changes.color
  WHEN NOT MATCHED THEN
    INSERT (carat, cut, color, price)
    VALUES (delta_changes.carat, delta_changes.cut, delta_changes.color, delta_changes.price)
"""

spark.sql(upsert_query)

# Read the updated Delta table
updated_df = spark.read.format("delta").load("/tmp/diamonds")

# Display the updated data
updated_df.show()


--------------------	 


